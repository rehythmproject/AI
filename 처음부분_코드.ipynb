{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7FdTa6d9wU2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"토큰값 입력하세요\")\n",
        "\n",
        "import torchaudio\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import re\n",
        "\n",
        "audio_file = \"/content/development_meeting_script.wav\"\n",
        "\n",
        "model_name = \"openai/whisper-medium\"\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "audio_input, sample_rate = torchaudio.load(audio_file)\n",
        "\n",
        "# 오디오 청크 처리 함수\n",
        "def transcribe_chunk(audio_chunk):\n",
        "    input_features = processor(\n",
        "        audio_chunk.squeeze(0),\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        language=\"ko\"\n",
        "    ).input_features\n",
        "    generated_ids = model.generate(input_features, max_new_tokens=300)\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return transcription[0]\n",
        "\n",
        "# 청크 길이 설정\n",
        "chunk_length = int(20 * sample_rate)\n",
        "num_chunks = (audio_input.shape[1] + chunk_length - 1) // chunk_length\n",
        "\n",
        "full_transcription = []\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    start = i * chunk_length\n",
        "    end = min((i + 1) * chunk_length, audio_input.shape[1])\n",
        "    audio_chunk = audio_input[:, start:end]\n",
        "    transcription = transcribe_chunk(audio_chunk)\n",
        "    full_transcription.append(transcription)\n",
        "\n",
        "text = \" \".join(full_transcription)\n",
        "formatted_text = re.sub(r'([.!?])\\s*', r'\\1\\n', text)\n",
        "\n",
        "print(formatted_text)\n",
        "\n",
        "with open(\"/content/formatted_text.txt\", \"w\") as f:\n",
        "    f.write(formatted_text)\n",
        "\n",
        "print(\"Transcription complete. Result saved to formatted_text.txt\")\n"
      ]
    }
  ]
}